{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIN-TAC-TOE ALPHA DRAFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m\u001b[1mInstalling dependencies from Pipfile‚Ä¶\u001b[39m\u001b[22m\n",
      "  üêç   \u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m 9/9 ‚Äî \u001b[30m\u001b[22m00:00:06\u001b[39m\u001b[22m[22m[22m22m[22m22m[22m22m[22m22m\n",
      "\u001b[0mInstalled kernelspec Python3.7-WinTacToe in /home/jan/.local/share/jupyter/kernels/python3.7-wintactoe\n",
      "\u001b[39m\u001b[22mShell for\u001b[39m\u001b[22m \u001b[32m\u001b[1m/home/jan/.local/share/virtualenvs/WinTacToe-0R3tXHGP\u001b[39m\u001b[22m \u001b[39m\u001b[1malready activated.\u001b[39m\u001b[22m\n",
      "No action taken to avoid nested environments.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!bash configure.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import sys\n",
    "sys.path.append('../extensions/games/tic_tac_toe')\n",
    "from tic_tac_toe_logic import *\n",
    "from tic_tac_toe import *\n",
    "\n",
    "import random\n",
    "from graphviz import Graph, Digraph\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RL Agent building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, array):\n",
    "        self.array = np.array(array)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.array.tostring())\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, State):\n",
    "            return NotImplemented\n",
    "        return hash(self) == hash(other)\n",
    "    \n",
    "    def __str__(self):\n",
    "#         return str(self.array)\n",
    "        representation = ''\n",
    "        height, width = self.array.shape\n",
    "        \n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                if self.array[h,w] == -1:\n",
    "                    representation += '#'\n",
    "                elif self.array[h,w] == 0:\n",
    "                    representation += 'O'\n",
    "                elif self.array[h,w] == 1:\n",
    "                    representation += 'X'\n",
    "                else:\n",
    "                    print(\"Invalid mark code\")\n",
    "                    raise\n",
    "            if h < height-1:\n",
    "                representation += '\\n'\n",
    "        return representation\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, array):\n",
    "        self.array = np.array(array)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.array.tostring())\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Action):\n",
    "            return NotImplemented\n",
    "        return hash(self) == hash(other)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.array)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, env):\n",
    "        self.policy_dict = {}\n",
    "    \n",
    "    # Lazy initialization\n",
    "    def __getitem__(self, state):\n",
    "        if self.policy_dict.get(state) is None:\n",
    "            if env.possible_actions:\n",
    "                self.policy_dict[state] = Action(random.choice(env.possible_actions))\n",
    "            else:\n",
    "                self.policy_dict[state] = Action([])\n",
    "        return self.policy_dict[state]\n",
    "    \n",
    "    def __setitem__(self, state, action):\n",
    "        self.policy_dict[state] = action\n",
    "    \n",
    "    def __str__(self):\n",
    "        representation = ''\n",
    "        for key, value in self.policy_dict.items():\n",
    "            representation += str(key)\n",
    "            representation += \"\\n\"\n",
    "            representation += str(value)\n",
    "            representation += \"\\n\\n\"\n",
    "        return representation\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def _repr_svg_(self):\n",
    "        graph = Digraph()\n",
    "        for state, action in self.policy_dict.items():\n",
    "            graph.attr('node', shape='doublecircle')\n",
    "            state_hash = str(hash(state))\n",
    "            graph.node(state_hash, str(state))\n",
    "            graph.attr('node', shape='circle')\n",
    "            action_hash = str(hash(action))\n",
    "            graph.node(action_hash, str(action))\n",
    "            graph.edge(state_hash, action_hash)\n",
    "                    \n",
    "        return graph._repr_svg_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = [Player('A', 0), Player('B', 1)]\n",
    "size = 3\n",
    "marks_required = 3\n",
    "env = gym.make('tic_tac_toe:tictactoe-v0')\n",
    "\n",
    "p = Policy(env)\n",
    "\n",
    "s1 = State([[-1,1],[-1,0]])\n",
    "a1 = Action([1,0])\n",
    "p[s1] = a1\n",
    "\n",
    "s2 = State([[-1,0],[1,0]])\n",
    "a2 = Action([0,0])\n",
    "p[s2] = a2\n",
    "\n",
    "s3 = State([[0,1],[1,-1]])\n",
    "a3 = Action([1,1])\n",
    "p[s3] = a3\n",
    "\n",
    "p[s3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValue:\n",
    "    def __init__(self):\n",
    "        self.action_value_dict = {}\n",
    "            \n",
    "    # Lazy initialization\n",
    "    def __getitem__(self, key):\n",
    "        if type(key) == tuple and len(key) == 2:\n",
    "            state, action = key\n",
    "            if self.action_value_dict.get(state) is None:\n",
    "                self.action_value_dict[state] = {action : 0} # Arbitrarily initialization\n",
    "            elif self.action_value_dict[state].get(action) is None:\n",
    "                self.action_value_dict[state][action] = 0 # Arbitrarily initialization\n",
    "                \n",
    "            return self.action_value_dict[state][action]\n",
    "        \n",
    "        elif type(key) == State:\n",
    "            return self.action_value_dict[key]\n",
    "    \n",
    "    def get_state_actions(self, state):\n",
    "        return self.action_value_dict.get(state)\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        state, action = key\n",
    "        if self.action_value_dict.get(state) is None:\n",
    "            self.action_value_dict[state] = {}\n",
    "        self.action_value_dict[state][action] = value\n",
    "        \n",
    "    # Argmax over action as argument, state is constant\n",
    "    # Settle draw randomly with uniform distribution\n",
    "    def argmax_a(self, state):\n",
    "        max_value = float('-inf')\n",
    "        max_value_actions = []\n",
    "        for action, value in self.action_value_dict[state].items():\n",
    "            if value > max_value:\n",
    "                max_value = value\n",
    "                max_value_actions = [action]\n",
    "            elif value == max_value:\n",
    "                max_value_actions.append(action)\n",
    "        return random.choice(max_value_actions)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.action_value_dict)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def _repr_svg_(self):\n",
    "        graph = Digraph()\n",
    "        for state, actions in self.action_value_dict.items():\n",
    "            graph.node(str(hash(state)), str(state))\n",
    "            for action, value in actions.items():\n",
    "                graph.node(str(hash(action)), str(action))\n",
    "                graph.edge(str(hash(state)), str(hash(action)), label=str(value))\n",
    "        return graph._repr_svg_()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action-value test\n",
    "\n",
    "av = ActionValue()\n",
    "\n",
    "s = State([[-1,-1],[-1,1]])\n",
    "\n",
    "a1 = Action([0,0])\n",
    "a2 = Action([0,1])\n",
    "a3 = Action([1,0])\n",
    "\n",
    "av[s,a1]=6\n",
    "av[s,a2]=2.9\n",
    "av[s,a3]=-10\n",
    "\n",
    "av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Returns:\n",
    "    def __init__(self):\n",
    "        self.returns_dict = {}\n",
    "    \n",
    "    # Lazy initialization\n",
    "    def __getitem__(self, key):\n",
    "        state, action = key\n",
    "        if self.returns_dict.get(state) is None:\n",
    "            self.returns_dict[state] = {action : []} # Arbitrarily initialization\n",
    "        elif self.returns_dict[state].get(action) is None:\n",
    "            self.returns_dict[state][action] = [] # Arbitrarily initialization\n",
    "            \n",
    "        return self.returns_dict[state][action]\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        state, action = key\n",
    "        if self.returns_dict.get(state) is None:\n",
    "            self.returns_dict[state] = {}\n",
    "        self.returns_dict[state][action] = value\n",
    "    \n",
    "    def _repr_svg_(self):\n",
    "        graph = Digraph()\n",
    "        graph.attr(rankdir=\"LR\")\n",
    "        for state, actions in self.returns_dict.items():\n",
    "            for action, returns in actions.items():\n",
    "                graph.attr('node', shape='doublecircle')\n",
    "                state_hash = str(uuid.uuid4())\n",
    "                graph.node(state_hash, str(state))\n",
    "                graph.attr('node', shape='circle')\n",
    "                action_hash = str(uuid.uuid4())\n",
    "                graph.node(action_hash, str(action))\n",
    "                graph.edge(state_hash, action_hash)\n",
    "                \n",
    "                graph.attr('node', shape='diamond')\n",
    "                last_hash = action_hash\n",
    "                for the_return in returns:\n",
    "                    return_hash = str(uuid.uuid4())\n",
    "                    graph.node(return_hash, str(the_return))\n",
    "                    graph.edge(last_hash, return_hash)\n",
    "                    last_hash = return_hash\n",
    "                    \n",
    "        return graph._repr_svg_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns test\n",
    "\n",
    "r = Returns()\n",
    "\n",
    "s1 = State([[-1,0],[-1,-1]])\n",
    "a1 = Action([1,0])\n",
    "G = -3\n",
    "r[s1,a1].append(G)\n",
    "\n",
    "s2 = State([[-1,0],[0,-1]])\n",
    "a2 = Action([0,0])\n",
    "G = 10.0\n",
    "r[s2,a2].append(G)\n",
    "\n",
    "s3 = State([[0,0],[0,-1]])\n",
    "a3 = Action([1,1])\n",
    "G = -5.1\n",
    "r[s3,a3].append(G)\n",
    "\n",
    "s4 = State([[-1,0],[0,0]])\n",
    "a4 = Action([0,0])\n",
    "G = -5.1\n",
    "r[s4,a4].append(G)\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode(list):\n",
    "    def __init__(self, the_list=[]):\n",
    "        super().__init__(the_list)\n",
    "\n",
    "    def __str__(self):\n",
    "        representation = ''\n",
    "        for element in self:\n",
    "            if type(element) == int:\n",
    "                representation += 'Reward:\\n'\n",
    "                representation += str(element)\n",
    "                representation += '\\n'\n",
    "            elif (type(element[0]), type(element[1])) == (State, Action):\n",
    "                representation += 'State:\\n'\n",
    "                representation += str(element[0])\n",
    "                representation += '\\n\\n'\n",
    "                representation += 'Action:\\n'\n",
    "                representation += str(element[1])\n",
    "                representation += '\\n'      \n",
    "            else:\n",
    "                print(\"Ivalid episode's element error\")\n",
    "                raise\n",
    "            representation += '\\n'\n",
    "        return representation\n",
    "    \n",
    "    def _repr_svg_(self):\n",
    "        graph = Digraph()\n",
    "        graph.attr(rankdir=\"LR\")\n",
    "        last_reward_hash = None\n",
    "        for i in range(int(len(self)/2)):\n",
    "            state = self[2*i][0]\n",
    "            action = self[2*i][1]\n",
    "            reward = self[2*i+1]\n",
    "                \n",
    "            # State node\n",
    "            graph.attr('node', shape='doublecircle')\n",
    "            state_hash = str(uuid.uuid4())\n",
    "            graph.node(state_hash, str(state))\n",
    "            \n",
    "            if last_reward_hash:\n",
    "                graph.edge(last_reward_hash, state_hash)\n",
    "                \n",
    "            # Action node\n",
    "            graph.attr('node', shape='circle')\n",
    "            action_hash = str(uuid.uuid4())\n",
    "            graph.node(action_hash, str(action))\n",
    "            graph.edge(state_hash, action_hash)\n",
    "            \n",
    "            # Next state node\n",
    "            graph.attr('node', shape='diamond')\n",
    "            reward_hash = str(uuid.uuid4())\n",
    "            graph.node(reward_hash, str(reward))\n",
    "            graph.edge(action_hash, reward_hash)\n",
    "            last_reward_hash = reward_hash\n",
    "        \n",
    "        return graph._repr_svg_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode test\n",
    "\n",
    "e = Episode()\n",
    "\n",
    "s1 = State([[-1,0],[-1,-1]])\n",
    "a1 = Action([1,0])\n",
    "e.append((s1,a1))\n",
    "r1 = -3\n",
    "e.append(r1)\n",
    "\n",
    "s2 = State([[-1,0],[0,-1]])\n",
    "a2 = Action([0,0])\n",
    "e.append((s2,a2))\n",
    "r2 = 4\n",
    "e.append(r2)\n",
    "\n",
    "s3 = State([[0,0],[0,-1]])\n",
    "a3 = Action([1,1])\n",
    "e.append((s3,a3))\n",
    "r3 = -8\n",
    "e.append(r2)\n",
    "\n",
    "s4 = State([[-1,0],[0,0]])\n",
    "a4 = Action([0,0])\n",
    "e.append((s4,a4))\n",
    "r4 = 10\n",
    "e.append(r4)\n",
    "\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.model_dict = {}\n",
    "    \n",
    "    # Lazy initialization\n",
    "    def __getitem__(self, key):\n",
    "        state, action = key\n",
    "        return self.model_dict.get(state, {}).get(action)\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        state, action = key\n",
    "        if self.model_dict.get(state) is None:\n",
    "            self.model_dict[state] = {}\n",
    "        self.model_dict[state][action] = value\n",
    "    \n",
    "    def _repr_svg_(self):\n",
    "        graph = Digraph()\n",
    "        for state, actions in self.model_dict.items():\n",
    "            for action, next_state in actions.items():\n",
    "                # State node\n",
    "                graph.attr('node', shape='doublecircle')\n",
    "                graph.attr('node', style='', color='', fontcolor = 'black')\n",
    "                state_hash = str(hash(state))\n",
    "                graph.node(state_hash, str(state))\n",
    "                \n",
    "                # Action node\n",
    "                graph.attr('node', shape='circle')\n",
    "                graph.attr('node', style='filled', color='black', fontcolor = 'white')\n",
    "                action_hash = str(hash(action))+state_hash\n",
    "                graph.node(action_hash, str(action))\n",
    "                graph.edge(state_hash, action_hash)\n",
    "                \n",
    "                # Next state node\n",
    "                graph.attr('node', shape='doublecircle')\n",
    "                graph.attr('node', style='', fontcolor = 'black')\n",
    "                next_state_hash = str(hash(next_state))\n",
    "                graph.node(next_state_hash, str(next_state))\n",
    "                graph.edge(action_hash, next_state_hash)\n",
    "        return graph._repr_svg_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model test\n",
    "\n",
    "m = Model()\n",
    "\n",
    "s1 = State([[-1,0],[-1,-1]])\n",
    "a1 = Action([1,0])\n",
    "s2 = State([[-1,0],[0,-1]])\n",
    "m[s1,a1] = s2\n",
    "\n",
    "a2 = Action([0,0])\n",
    "s3 = State([[0,0],[0,-1]])\n",
    "m[s2,a2] = s3\n",
    "\n",
    "a3 = Action([1,1])\n",
    "s4 = State([[-1,0],[0,0]])\n",
    "m[s2,a3] = s4\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, model, action_value, policy):\n",
    "        self.model = model\n",
    "        self.action_value = action_value\n",
    "        self.policy = policy\n",
    "        \n",
    "        mdp_graph = Digraph()\n",
    "        \n",
    "        model_dict = self.model.model_dict\n",
    "        for state, actions in model_dict.items():\n",
    "            for action, next_state in actions.items():\n",
    "                # State node\n",
    "                mdp_graph.attr('node', shape='doublecircle', style='', fontcolor = 'black')\n",
    "                state_hash = str(hash(state))\n",
    "                mdp_graph.node(state_hash, str(state))\n",
    "                \n",
    "                # Action node\n",
    "                mdp_graph.attr('node', shape='circle', style='filled', color='black',\n",
    "                               fontcolor = 'white')\n",
    "                action_hash = str(hash(action))+state_hash\n",
    "                mdp_graph.node(action_hash, str(action))\n",
    "                this_action_value = self.action_value[state, action]\n",
    "                if self.policy[state] == action:\n",
    "                    this_action_color = 'red'\n",
    "                else:\n",
    "                    this_action_color = 'black'\n",
    "                mdp_graph.edge(state_hash,\n",
    "                               action_hash,\n",
    "                               label=str(this_action_value),\n",
    "                               color = this_action_color)\n",
    "                \n",
    "                # Next state node\n",
    "                mdp_graph.attr('node', shape='doublecircle', style='', fontcolor = 'black')\n",
    "                next_state_hash = str(hash(next_state))\n",
    "                mdp_graph.node(next_state_hash, str(next_state))\n",
    "                mdp_graph.edge(action_hash, next_state_hash)\n",
    "        \n",
    "        self.mdp_graph = mdp_graph\n",
    "    \n",
    "    def _repr_svg_(self):\n",
    "        return self.mdp_graph._repr_svg_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment, player):\n",
    "        self.player = player\n",
    "        self.env = environment\n",
    "        \n",
    "        self.policy = Policy(self.env)\n",
    "        self.action_value = ActionValue()\n",
    "        self.returns = Returns()\n",
    "        self.last_episode = Episode()\n",
    "        self.model = Model()\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.last_MDP = None\n",
    "    \n",
    "    def step(self, state, epsilon):\n",
    "\n",
    "        # Choose action in epsilon-greedy way\n",
    "        if np.random.rand() >= epsilon:\n",
    "            action = self.policy[state]\n",
    "        else:\n",
    "            if self.env.possible_actions:\n",
    "                action = Action(random.choice(self.env.possible_actions))\n",
    "                self.policy[state] = action\n",
    "            else:\n",
    "                action = Action([])\n",
    "                \n",
    "        self.last_episode.append((state, action)) # Register state and action\n",
    "        raw_state, reward, done, _ = env.step(action.array, self.player) # Take action\n",
    "        \n",
    "        # Register model transition\n",
    "        if self.last_state and self.last_action:\n",
    "            self.model[self.last_state, self.last_action] = state\n",
    "        self.last_state, self.last_action = state, action\n",
    "\n",
    "        self.last_episode.append(reward) # Register reward\n",
    "        return State(raw_state), done\n",
    "    \n",
    "    def random_step(self, state):\n",
    "        action = Action(random.choice(self.env.possible_actions)) # Choose action\n",
    "        self.last_episode.append((state, action)) # Register state and action\n",
    "        raw_state, reward, done, _ = env.step(action.array, self.player) # Take action\n",
    "        \n",
    "        # Register model transition\n",
    "        if self.last_state and self.last_action:\n",
    "            self.model[self.last_state, self.last_action] = state\n",
    "        self.last_state, self.last_action = state, action\n",
    "        \n",
    "        self.last_episode.append(reward) # Register reward\n",
    "        return State(raw_state), done\n",
    "    \n",
    "    def get_MDP(self):\n",
    "        self.last_MDP = MDP(self.model, self.action_value, self.policy)\n",
    "        return self.last_MDP\n",
    "    \n",
    "    def reset_policy(self):\n",
    "        self.policy = Policy(self.env)\n",
    "    \n",
    "    def reset_action_value(self):\n",
    "        self.action_value = ActionValue()\n",
    "    \n",
    "    def reset_returns(self):\n",
    "        self.returns = Returns()\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.last_episode = Episode()\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "    \n",
    "    def reset_model(self):\n",
    "        self.model = Model()\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "    \n",
    "    def reset_agent():\n",
    "        self.reset_policy()\n",
    "        self.reset_action_value()\n",
    "        self.reset_returns()\n",
    "        self.reset_episode()\n",
    "        self.reset_model()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo with Exploring Starts algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloES:\n",
    "    def __init__(self, agent1, agent2):\n",
    "        self.agent1 = agent1\n",
    "        self.agent2 = agent2\n",
    "        self.agent1_G = []\n",
    "        self.agent2_G = []\n",
    "        self.epsilons = []\n",
    "        \n",
    "    def train(self, episodes_no=1, initial_epsilon=0.3):\n",
    "        for i in range(episodes_no):\n",
    "            epsilon = np.e**(-i/episodes_no*5) * initial_epsilon # Epsilon decaying\n",
    "            self.epsilons.append(epsilon) # Register epsilon for plotting purpose\n",
    "            \n",
    "            self.gen_episode(self.agent1, self.agent2, epsilon)\n",
    "            self.agent1_G.append(self.pass_episode(self.agent1))\n",
    "            self.agent2_G.append(self.pass_episode(self.agent2))\n",
    "    \n",
    "    def gen_episode(self, agent1, agent2, epsilon):\n",
    "        # Reset agents' episodes\n",
    "        agents = [agent1, agent2]\n",
    "        for a in agents:\n",
    "            a.reset_episode()\n",
    "\n",
    "        # Players queue\n",
    "        agents = cycle(agents)\n",
    "        current_agent = next(agents)\n",
    "\n",
    "        # Initialize environment\n",
    "        env = agent1.env\n",
    "        players = [Player('A', 0), Player('B', 1)]\n",
    "        size = 3\n",
    "        marks_required = 3\n",
    "        env.initialize(players, size, marks_required)\n",
    "\n",
    "        # Generate episode\n",
    "        done = False\n",
    "        initial_state = State(env.random_initial_state()) # Random state due to Exploring Starts approach\n",
    "        state, done = current_agent.random_step(initial_state) # Random action due to Exploring Starts approach\n",
    "        while not done:\n",
    "            current_agent = next(agents)\n",
    "            state, done = current_agent.step(state, epsilon)\n",
    "\n",
    "#         current_agent = next(agents)\n",
    "    \n",
    "    def pass_episode(self, agent):\n",
    "        episode = agent.last_episode\n",
    "        gamma = 0.9                         # Discount factor\n",
    "        G = 0                               # Episode's accumulative discounted total reward/return\n",
    "        steps_no = len(episode)//2\n",
    "        for t in reversed(range(steps_no)):\n",
    "            S, A = episode[2*t]             # This step's (state, action) pair\n",
    "            R = episode[2*t+1]              # This step's reward\n",
    "\n",
    "            G = gamma * G + R               # Calculate discounted return\n",
    "            \n",
    "            # Update rule according to the Monte Carlo first-step approach\n",
    "            if not (S,A) in episode[0:2*t]:\n",
    "                # Policy evaluation\n",
    "                agent.returns[S,A].append(G)\n",
    "                agent.action_value[S,A] = np.mean(agent.returns[S,A])\n",
    "                # Greedy policy improvement\n",
    "                agent.policy[S] = agent.action_value.argmax_a(S)\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "env = gym.make('tic_tac_toe:tictactoe-v0')\n",
    "agent1 = Agent(env, players[0])\n",
    "agent2 = Agent(env, players[1])\n",
    "mces = MonteCarloES(agent1, agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "mces.train(100, initial_epsilon=0.3)\n",
    "\n",
    "# Plot \n",
    "x = list(range(len(mces.agent2_G)))\n",
    "y = mces.agent2_G\n",
    "plt.scatter(x, y)\n",
    "plt.ylabel('Obtained Return')\n",
    "plt.xlabel('Episodes')\n",
    "print(\"Returns vs episodes:\")\n",
    "plt.show()\n",
    "\n",
    "x = np.array(mces.agent2_G)[-1000:-1]\n",
    "n, bins, patches = plt.hist(x, bins='auto', density=True, facecolor='orange', alpha=1)\n",
    "plt.xlabel('Returns')\n",
    "plt.ylabel('Number of returns')\n",
    "plt.title('Histogram of Returns')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(mces.epsilons)\n",
    "plt.ylabel('Epsilon value')\n",
    "plt.ylabel('Epsilon value')\n",
    "print(\"Epsilones vs episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = agent1.get_MDP()\n",
    "mdp.mdp_graph.view()\n",
    "# mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last episode\")\n",
    "agent1.last_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action-value function\")\n",
    "agent1.action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Policy\")\n",
    "agent1.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Returns for (State, Action) pairs\")\n",
    "agent1.returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = []\n",
    "# for state, actions in agent1.returns.returns_dict.items():\n",
    "#     for action, returns in actions.items():\n",
    "#         lens.append(len(returns))\n",
    "#         if len(returns) >= 10:\n",
    "#             print(\"State:\")\n",
    "#             print(state)\n",
    "#             print(f\"Action: {action}\")\n",
    "#             print(f\"Returns: {returns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Model\")\n",
    "agent1.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "# import sys\n",
    "# sys.path.append('/home/jan/WinTacToe/src/extensions/games/tic_tac_toe')\n",
    "# from tic_tac_toe_logic import *\n",
    "# players = [Player('A', 0), Player('B', 1)]\n",
    "# size = 3\n",
    "# marks_required = 3\n",
    "# env = gym.make('tic_tac_toe:tictactoe-v0')\n",
    "# env.initialize(players, size, marks_required)\n",
    "# s = env.random_initial_state()\n",
    "# State(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- [ ] Zagraƒá przeciwko randomowemu graczowi\n",
    "- [ ] Zaimplementowaƒá Human Player Agent\n",
    "- [X] zmieniƒá episode na last_episode w klasie Agent\n",
    "- [ ] Rozegraƒá gry ze standardowym stanem poczƒÖtkowym\n",
    "- [X] Dodaƒá MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7-WinTacToe",
   "language": "python",
   "name": "python3.7-wintactoe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
