---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.2.1
  kernelspec:
    display_name: Python3.7-WinTacToe
    language: python
    name: python3.7-wintactoe
---

# WIN-TAC-TOE ALPHA DRAFT


### WARRING: Read before run

In case of import errors or unable-of-editing-this-notebook:
1. Make sure you have executed "pipenv install --skip-lock" command in project root directory
2. Make sure you have started project shell with "pipenv shell" BEFORE launching "jupyter notebook"
3. Run first cell below 
4. Refresh jupyter notebook page in your browser and next choose Kernel-> Change kernel -> Python3.7-WinTacToe

```{python}
# !python -m ipykernel install --user --name=Python3.7-WinTacToe
```

In case of having no graphviz backend installed (or just graphviz error):

```{python}
import getpass
import os
print("Type linux user account password:")
password = getpass.getpass()
command = "sudo -S apt install graphviz"
if os.system('echo %s | %s' % (password, command)) == 0:
    print("Installation successfull")
else:
    print("Something went wrong")
```

### Import section

```{python}
import numpy as np
import gym

import sys
sys.path.append('../extensions/games/tic_tac_toe')
from tic_tac_toe_logic import *
from tic_tac_toe import *

import random
from graphviz import Graph, Digraph
import matplotlib.pyplot as plt
import uuid
from parse import parse
```

### Utilities

```{python}
def linear_map(value, low, high, values):
    original_high = max(values)
    original_low = min(values)
    if original_high == original_low:
        return high
    original_midpoint = (original_high + original_low)/2
    original_halfrange = original_midpoint - original_low
    value = (value - original_midpoint)/original_halfrange
    
    midpoint = (high + low)/2
    halfrange = midpoint - low
    value = midpoint + value * halfrange
    return value
```

### Basic RL Agent building blocks

```{python}
class State:
    def __init__(self, array):
        self.array = np.array(array)
    
    def __hash__(self):
        return hash(self.array.tostring())
    
    def __eq__(self, other):
        if not isinstance(other, State):
            return NotImplemented
        return hash(self) == hash(other)
    
    def __str__(self):
        representation = ''
        height, width = self.array.shape
        
        for h in range(height):
            for w in range(width):
                if self.array[h,w] == -1:
                    representation += '#'
                elif self.array[h,w] == 0:
                    representation += 'O'
                elif self.array[h,w] == 1:
                    representation += 'X'
                else:
                    print("Invalid mark code")
                    raise
            if h < height-1:
                representation += '\n'
        return representation
        
    
    def __repr__(self):
        return self.__str__()
```

```{python}
class Action:
    def __init__(self, array):
        self.array = np.array(array)
    
    def __hash__(self):
        return hash(self.array.tostring())
    
    def __eq__(self, other):
        if not isinstance(other, Action):
            return NotImplemented
        return hash(self) == hash(other)
    
    def __str__(self):
        return str(self.array)
    
    def __repr__(self):
        return self.__str__()
```

```{python}
class Policy:
    def __init__(self, env):
        self.policy_dict = {}
        self.env = env
    
    # Lazy initialization
    def __getitem__(self, state):
        if self.policy_dict.get(state) is None:
            if self.env.possible_actions:
                self.policy_dict[state] = Action(random.choice(self.env.possible_actions))
            else:
                self.policy_dict[state] = Action([])
        return self.policy_dict[state]
    
    def __setitem__(self, state, action):
        self.policy_dict[state] = action
    
    def __str__(self):
        representation = ''
        for key, value in self.policy_dict.items():
            representation += str(key)
            representation += "\n"
            representation += str(value)
            representation += "\n\n"
        return representation
    def __repr__(self):
        return self.__str__()
    
    def _repr_svg_(self):
        graph = Digraph()
        for state, action in self.policy_dict.items():
            graph.attr('node', shape='doublecircle')
            state_hash = str(hash(state))
            graph.node(state_hash, str(state))
            graph.attr('node', shape='circle')
            action_hash = str(hash(action))
            graph.node(action_hash, str(action))
            graph.edge(state_hash, action_hash)
                    
        return graph._repr_svg_()
```

```{python}
players = [Player('A', 0), Player('B', 1)]
size = 3
marks_required = 3
env = gym.make('tic_tac_toe:tictactoe-v0')

p = Policy(env)

s1 = State([[-1,1],[-1,0]])
a1 = Action([1,0])
p[s1] = a1

s2 = State([[-1,0],[1,0]])
a2 = Action([0,0])
p[s2] = a2

s3 = State([[0,1],[1,-1]])
a3 = Action([1,1])
p[s3] = a3

p
```

```{python}
class ActionValue:
    MIN_PEN_WIDTH = 1
    MAX_PEN_WIDTH = 4
    
    def __init__(self):
        self.action_value_dict = {}
            
    # Lazy initialization
    def __getitem__(self, key):
        if type(key) == tuple and len(key) == 2:
            state, action = key
            if self.action_value_dict.get(state) is None:
                self.action_value_dict[state] = {action : 0} # Arbitrarily initialization
            elif self.action_value_dict[state].get(action) is None:
                self.action_value_dict[state][action] = 0 # Arbitrarily initialization
                
            return self.action_value_dict[state][action]
        
        elif type(key) == State:
            return self.action_value_dict[key]
    
    def get_state_actions(self, state):
        return self.action_value_dict.get(state)
    
    def __setitem__(self, key, value):
        state, action = key
        if self.action_value_dict.get(state) is None:
            self.action_value_dict[state] = {}
        self.action_value_dict[state][action] = value
        
    # Argmax over action as argument, state is constant
    # Settle draw randomly with uniform distribution
    def argmax_a(self, state):
        max_value = float('-inf')
        max_value_actions = []
        for action, value in self.action_value_dict.get(state, {}).items():
            if value > max_value:
                max_value = value
                max_value_actions = [action]
            elif value == max_value:
                max_value_actions.append(action)
        if max_value_actions:
            return random.choice(max_value_actions)
        else:
            return Action([])
    
    def __str__(self):
        return str(self.action_value_dict)
    
    def __repr__(self):
        return self.__str__()
    
    def _repr_svg_(self):
        graph = Digraph()
        for state, actions in self.action_value_dict.items():
            # Calculate sum of all actions' values from this state
      
            graph.node(str(hash(state)), str(state))
            for action, value in actions.items():
                graph.node(str(hash(action)), str(action))
                red = int(linear_map(value, 0, 255, actions.values()))
                color = '#%02x%02x%02x' % (red, 0, 0)
                penwidth = str(linear_map(value, self.MIN_PEN_WIDTH, self.MAX_PEN_WIDTH, actions.values()))
                graph.edge(str(hash(state)), str(hash(action)), label=str(value),
                           color=color, penwidth=penwidth)
        return graph._repr_svg_()       
```

```{python}
# Action-value test

av = ActionValue()

s = State([[-1,-1],[-1,1]])

a1 = Action([0,0])
a2 = Action([0,1])
a3 = Action([1,0])

av[s,a1]=6
av[s,a2]=0.8
av[s,a3]=-10

av
```

```{python}
class EpsilonGreedyPolicy:
    def __init__(self, env, action_value, epsilon):
        self.action_value = action_value
        self.env = env
        self.epsilon = epsilon
        
    # Action-value and epsilon based action choosing
    def __getitem__(self, state):
        if random.random() >= self.epsilon:
            action =  self.action_value.argmax_a(state)
            if action != Action([]):
                return action
        if self.env.possible_actions:
            return Action(random.choice(self.env.possible_actions))
        return Action([])
    
    def __str__(self):
        return self.action_value.__str__()
        
    def __repr__(self):
        return self.__str__()
    
    def _repr_svg_(self):
        return self.action_value._repr_svg_()
```

```{python}
players = [Player('A', 0), Player('B', 1)]
size = 3
marks_required = 3
env = gym.make('tic_tac_toe:tictactoe-v0')
env.initialize(players, size, marks_required)
s = env.random_initial_state()

av = ActionValue()

s = State([[-1,-1],[-1,1]])

a1 = Action([0,0])
a2 = Action([0,1])
a3 = Action([1,0])

av[s,a1]=6
av[s,a2]=2.9
av[s,a3]=-10

egp = EpsilonGreedyPolicy(env, av, 0.3)
egp
```

```{python}
class Returns:
    def __init__(self):
        self.returns_dict = {}
    
    # Lazy initialization
    def __getitem__(self, key):
        state, action = key
        if self.returns_dict.get(state) is None:
            self.returns_dict[state] = {action : []} # Arbitrarily initialization
        elif self.returns_dict[state].get(action) is None:
            self.returns_dict[state][action] = [] # Arbitrarily initialization
            
        return self.returns_dict[state][action]
    
    def __setitem__(self, key, value):
        state, action = key
        if self.returns_dict.get(state) is None:
            self.returns_dict[state] = {}
        self.returns_dict[state][action] = value
    
    def _repr_svg_(self):
        graph = Digraph()
        graph.attr(rankdir="LR")
        for state, actions in self.returns_dict.items():
            for action, returns in actions.items():
                graph.attr('node', shape='doublecircle')
                state_hash = str(uuid.uuid4())
                graph.node(state_hash, str(state))
                graph.attr('node', shape='circle')
                action_hash = str(uuid.uuid4())
                graph.node(action_hash, str(action))
                graph.edge(state_hash, action_hash)
                
                graph.attr('node', shape='diamond')
                last_hash = action_hash
                for the_return in returns:
                    return_hash = str(uuid.uuid4())
                    graph.node(return_hash, str(the_return))
                    graph.edge(last_hash, return_hash)
                    last_hash = return_hash
                    
        return graph._repr_svg_()
```

```{python}
# Returns test

r = Returns()

s1 = State([[-1,0],[-1,-1]])
a1 = Action([1,0])
G = -3
r[s1,a1].append(G)

s2 = State([[-1,0],[0,-1]])
a2 = Action([0,0])
G = 10.0
r[s2,a2].append(G)

s3 = State([[0,0],[0,-1]])
a3 = Action([1,1])
G = -5.1
r[s3,a3].append(G)

s4 = State([[-1,0],[0,0]])
a4 = Action([0,0])
G = -5.1
r[s4,a4].append(G)

r
```

```{python}
class Episode(list):
    def __init__(self, the_list=[]):
        super().__init__(the_list)

    def __str__(self):
        representation = ''
        for element in self:
            if type(element) == int:
                representation += 'Reward:\n'
                representation += str(element)
                representation += '\n'
            elif (type(element[0]), type(element[1])) == (State, Action):
                representation += 'State:\n'
                representation += str(element[0])
                representation += '\n\n'
                representation += 'Action:\n'
                representation += str(element[1])
                representation += '\n'      
            else:
                print("Ivalid episode's element error")
                raise
            representation += '\n'
        return representation
    
    def _repr_svg_(self):
        graph = Digraph()
        graph.attr(rankdir="LR")
        last_reward_hash = None
        for i in range(int(len(self)/2)):
            state = self[2*i][0]
            action = self[2*i][1]
            reward = self[2*i+1]
                
            # State node
            graph.attr('node', shape='doublecircle')
            state_hash = str(uuid.uuid4())
            graph.node(state_hash, str(state))
            
            if last_reward_hash:
                graph.edge(last_reward_hash, state_hash)
                
            # Action node
            graph.attr('node', shape='circle')
            action_hash = str(uuid.uuid4())
            graph.node(action_hash, str(action))
            graph.edge(state_hash, action_hash)
            
            # Next state node
            graph.attr('node', shape='diamond')
            reward_hash = str(uuid.uuid4())
            graph.node(reward_hash, str(reward))
            graph.edge(action_hash, reward_hash)
            last_reward_hash = reward_hash
        
        return graph._repr_svg_()
```

```{python}
# Episode test

e = Episode()

s1 = State([[-1,0],[-1,-1]])
a1 = Action([1,0])
e.append((s1,a1))
r1 = -3
e.append(r1)

s2 = State([[-1,0],[0,-1]])
a2 = Action([0,0])
e.append((s2,a2))
r2 = 4
e.append(r2)

s3 = State([[0,0],[0,-1]])
a3 = Action([1,1])
e.append((s3,a3))
r3 = -8
e.append(r2)

s4 = State([[-1,0],[0,0]])
a4 = Action([0,0])
e.append((s4,a4))
r4 = 10
e.append(r4)

e
```

```{python}
class Model:
    def __init__(self):
        self.model_dict = {}
    
    # Lazy initialization
    def __getitem__(self, key):
        state, action = key
        return self.model_dict.get(state, {}).get(action)
    
    def __setitem__(self, key, value):
        state, action = key
        if self.model_dict.get(state) is None:
            self.model_dict[state] = {}
        self.model_dict[state][action] = value
    
    def _repr_svg_(self):
        graph = Digraph()
        for state, actions in self.model_dict.items():
            for action, next_state in actions.items():
                # State node
                graph.attr('node', shape='doublecircle')
                graph.attr('node', style='', color='', fontcolor = 'black')
                state_hash = str(hash(state))
                graph.node(state_hash, str(state))
                
                # Action node
                graph.attr('node', shape='circle')
                graph.attr('node', style='filled', color='black', fontcolor = 'white')
                action_hash = str(hash(action))+state_hash
                graph.node(action_hash, str(action))
                graph.edge(state_hash, action_hash)
                
                # Next state node
                graph.attr('node', shape='doublecircle')
                graph.attr('node', style='', fontcolor = 'black')
                next_state_hash = str(hash(next_state))
                graph.node(next_state_hash, str(next_state))
                graph.edge(action_hash, next_state_hash)
        return graph._repr_svg_()
```

```{python}
# Model test

m = Model()

s1 = State([[-1,0],[-1,-1]])
a1 = Action([1,0])
s2 = State([[-1,0],[0,-1]])
m[s1,a1] = s2

a2 = Action([0,0])
s3 = State([[0,0],[0,-1]])
m[s2,a2] = s3

a3 = Action([1,1])
s4 = State([[-1,0],[0,0]])
m[s2,a3] = s4

m
```

```{python}
class StochasticModel:
    MIN_PEN_WIDTH = 1
    MAX_PEN_WIDTH = 4
    def __init__(self):
        self.model_dict = {}
    
    # Lazy initialization
    def __getitem__(self, key):
        state, action = key
        return self.model_dict.get(state, {}).get(action, {})
    
    def __setitem__(self, key, next_state):
        state, action = key
        if self.model_dict.get(state) is None:
            self.model_dict[state] = {}
        if self.model_dict[state].get(action) is None: 
            self.model_dict[state][action] = {}
        if self.model_dict[state][action].get(next_state) is None:
            self.model_dict[state][action][next_state] = 0
        self.model_dict[state][action][next_state] += 1
    
    def _repr_svg_(self):
        graph = Digraph()
        for state, actions in self.model_dict.items():
            # State node
            graph.attr('node', shape='doublecircle')
            graph.attr('node', style='', color='', fontcolor = 'black')
            state_hash = str(hash(state))
            graph.node(state_hash, str(state))
            for action, next_states in actions.items():
                # Action node
                graph.attr('node', shape='circle')
                graph.attr('node', style='filled', color='black', fontcolor = 'white')
                action_hash = str(hash(action))+state_hash
                graph.node(action_hash, str(action))
                graph.edge(state_hash, action_hash)
                
                # Calculate sum of all visits numbers
                all_visits_no = sum(next_states.values())

                for next_state, visits_number in next_states.items():
                    # Next state node
                    graph.attr('node', shape='doublecircle')
                    graph.attr('node', style='', fontcolor = 'black')
                    next_state_hash = str(hash(next_state))
                    graph.node(next_state_hash, str(next_state))
                    visits_percentage = np.round(visits_number/all_visits_no*100, 2)
                    label = f"{visits_number} ({visits_percentage}%)"
                    blue = int(linear_map(visits_number, 0, 255, next_states.values()))
                    color = '#%02x%02x%02x' % (0, 0, blue)
                    penwidth = str(linear_map(visits_number, self.MIN_PEN_WIDTH,
                                              self.MAX_PEN_WIDTH, next_states.values()))
                    graph.edge(action_hash, next_state_hash, label=label,
                               color=color, penwidth=penwidth)
        return graph._repr_svg_()
```

```{python}
# Model test

m = StochasticModel()

s1 = State([[-1,0],[-1,-1]])
a1 = Action([1,0])
s2 = State([[-1,0],[0,-1]])
s3 = State([[-1,0],[-1,0]])
m[s1,a1] = s2
m[s1,a1] = s3
m[s1,a1] = s3
m[s1,a1] = s3
m[s1,a1] = s3

a2 = Action([0,0])
s4 = State([[0,0],[0,-1]])
m[s2,a2] = s4

a4 = Action([1,1])
s5 = State([[-1,0],[0,0]])
s6 = State([[-1,0],[-1,0]])
s7 = State([[-1,-1],[-1,-1]])
m[s2,a4] = s5
m[s2,a4] = s5
m[s2,a4] = s5
m[s2,a4] = s5
m[s2,a4] = s5
m[s2,a4] = s5
m[s2,a4] = s5
m[s2,a4] = s5
m[s2,a4] = s6
m[s2,a4] = s6
m[s2,a4] = s6
m[s2,a4] = s6
m[s2,a4] = s6
m[s2,a4] = s7
m[s2,a4] = s7
m[s2,a4] = s7

m
```

```{python}
class MDP:
    MIN_PEN_WIDTH = 1
    MAX_PEN_WIDTH = 4
    def __init__(self, model, action_value, policy):
        self.model = model
        self.action_value = action_value
        
        mdp_graph = Digraph()
        
        model_dict = self.model.model_dict
        for state, actions in model_dict.items():
            # State node
            mdp_graph.attr('node', shape='doublecircle', style='', fontcolor = 'black')
            state_hash = str(hash(state))
            mdp_graph.node(state_hash, str(state))
            for action, next_states in actions.items():
                # Action node
                mdp_graph.attr('node', shape='circle', style='filled', color='black',
                               fontcolor = 'white')
                action_hash = str(hash(action))+state_hash
                mdp_graph.node(action_hash, str(action))
                
                this_action_value = np.round(self.action_value[state, action],2)
                action_values = self.action_value.action_value_dict[state].values()
                red = int(linear_map(this_action_value, 0, 255, action_values))
                color = '#%02x%02x%02x' % (red, 0, 0)
                penwidth = str(linear_map(this_action_value, self.MIN_PEN_WIDTH,
                               self.MAX_PEN_WIDTH, action_values))
                mdp_graph.edge(state_hash,
                               action_hash,
                               label=str(this_action_value),
                               color = color, penwidth=penwidth)
                
                # Calculate sum of all visits numbers
                all_visits_no = sum(next_states.values())
                for next_state, visits_number in next_states.items():
                    # Next state node
                    mdp_graph.attr('node', shape='doublecircle', style='', fontcolor = 'black')
                    next_state_hash = str(hash(next_state))
                    mdp_graph.node(next_state_hash, str(next_state))
                    
                    visits_percentage = np.round(visits_number/all_visits_no*100, 2)
                    label = f"{visits_number} ({visits_percentage}%)"
                    blue = int(linear_map(visits_number, 0, 255, next_states.values()))
                    color = '#%02x%02x%02x' % (0, 0, blue)
                    penwidth = str(linear_map(visits_number, self.MIN_PEN_WIDTH, self.MAX_PEN_WIDTH,
                                   next_states.values()))
                    mdp_graph.edge(action_hash, next_state_hash, label=label,
                                   color=color, penwidth=penwidth)
        
        self.mdp_graph = mdp_graph
    
    def _repr_svg_(self):
        return self.mdp_graph._repr_svg_()
```

### RL Agent

```{python}
class Agent:
    def __init__(self, environment, player):
        self.player = player
        self.env = environment
        

        self.action_value = ActionValue()
        self.policy = EpsilonGreedyPolicy(self.env, self.action_value, 0.3)
        self.returns = Returns()
        self.last_episode = Episode()
        self.model = StochasticModel()
        self.last_state = None
        self.last_action = None
        self.last_MDP = None
    
    def step(self, state, epsilon):

        # Choose action in epsilon-greedy way
        self.policy.epsilon = epsilon
        action = self.policy[state]
        self.last_episode.append((state, action)) # Register state and action
        raw_state, reward, done, _ = env.step(action.array, self.player) # Take action
        
        # Register model transition
        if self.last_state and self.last_action:
            self.model[self.last_state, self.last_action] = state
        self.last_state, self.last_action = state, action

        self.last_episode.append(reward) # Register reward
        return State(raw_state), done
    
    def random_step(self, state):
        action = Action(random.choice(self.env.possible_actions)) # Choose action
        self.last_episode.append((state, action)) # Register state and action
        raw_state, reward, done, _ = env.step(action.array, self.player) # Take action
        
        # Register model transition
        if self.last_state and self.last_action:
            self.model[self.last_state, self.last_action] = state
        self.last_state, self.last_action = state, action
        
        self.last_episode.append(reward) # Register reward
        return State(raw_state), done
    
    def get_MDP(self):
        self.last_MDP = MDP(self.model, self.action_value, self.policy)
        return self.last_MDP
    
    def reset_policy(self):
        self.policy = Policy(self.env)
    
    def reset_action_value(self):
        self.action_value = ActionValue()
    
    def reset_returns(self):
        self.returns = Returns()

    def reset_episode(self):
        self.last_episode = Episode()
        self.last_state = None
        self.last_action = None
    
    def reset_model(self):
        self.model = Model()
        self.last_state = None
        self.last_action = None
    
    def reset_agent():
        self.reset_policy()
        self.reset_action_value()
        self.reset_returns()
        self.reset_episode()
        self.reset_model()
        
```

### Monte Carlo with Exploring Starts algorithm implementation

```{python}
class MonteCarloES:
    def __init__(self, agent1, agent2):
        self.agent1 = agent1
        self.agent2 = agent2
        self.agent1_G = []
        self.agent2_G = []
        self.epsilons = []
        
    def train(self, episodes_no=1, initial_epsilon=0.3):
        for i in range(episodes_no):
            epsilon = np.e**(-i/episodes_no*5) * initial_epsilon # Epsilon decaying
            self.epsilons.append(epsilon) # Register epsilon for plotting purpose
            
            self.gen_episode(self.agent1, self.agent2, epsilon)
            self.agent1_G.append(self.pass_episode(self.agent1))
            self.agent2_G.append(self.pass_episode(self.agent2))
    
    def gen_episode(self, agent1, agent2, epsilon):
        # Reset agents' episodes
        agents = [agent1, agent2]
        for a in agents:
            a.reset_episode()

        # Players queue
        agents = cycle(agents)
        current_agent = next(agents)

        # Initialize environment
        env = agent1.env
        players = [agent1.player, agent2.player]
        size = 3
        marks_required = 3
        env.initialize(players, size, marks_required)

        # Generate episode
        done = False
        initial_state = State(env.get_current_state())
#         initial_state = State(env.random_initial_state()) # Random state due to Exploring Starts approach
        state, done = current_agent.random_step(initial_state) # Random action due to Exploring Starts approach
        while not done:
            current_agent = next(agents)
            state, done = current_agent.step(state, epsilon)

#         current_agent = next(agents)
    
    def pass_episode(self, agent):
        episode = agent.last_episode
        gamma = 0.9                         # Discount factor
        G = 0                               # Episode's accumulative discounted total reward/return
        steps_no = len(episode)//2
        for t in reversed(range(steps_no)):
            S, A = episode[2*t]             # This step's (state, action) pair
            R = episode[2*t+1]              # This step's reward

            G = gamma * G + R               # Calculate discounted return
            
            # Update rule according to the Monte Carlo first-step approach
            if not (S,A) in episode[0:2*t]:
                # Policy evaluation
                agent.returns[S,A].append(G)
#                 # Improve only policy of the first agent
#                 if agent.player.name == agent1.player.name:
#                     agent.action_value[S,A] = np.mean(agent.returns[S,A])
                agent.action_value[S,A] = np.mean(agent.returns[S,A]) # Improve policy of both agents
                # Greedy policy improvement in the background (as a consequence of action_value change)
#                 agent.policy[S] = agent.action_value.argmax_a(S)
        return G
```

### Training

```{python}
# Initialization
env = gym.make('tic_tac_toe:tictactoe-v0')
players = [Player('A', 0), Player('B', 1)]
agent1 = Agent(env, players[0])
agent2 = Agent(env, players[1])
# agent2.policy = Policy(env) # Fixed, random policy
mces = MonteCarloES(agent1, agent2)
```

```{python}
# Train
mces.train(150000, initial_epsilon=0.3)
```

```{python}
# Plot 
agent_data = mces.agent1_G

bin_size = 100
bins_number = int(np.floor(len(agent_data)/bin_size))
mean_returns = [np.mean(agent_data[i*bin_size:(i+1)*bin_size]) for i in range(bins_number)]
mean_returns.append(np.mean(agent_data[bins_number*bin_size:]))

x = list(range(len(mean_returns)))
y = mean_returns
plt.scatter(x, y)
plt.ylabel(f'Mean Returns')
plt.xlabel('Episode bins (bin size: {bin_size})')
print("Mean Returns vs episodes:")
plt.show()

x = np.array(agent_data)[-1000:-1]
n, bins, patches = plt.hist(x, bins='auto', density=True, facecolor='orange', alpha=1)
plt.xlabel('Return value')
plt.ylabel('Number of returns')
plt.title('Histogram of last 1000 Returns')
plt.grid(True)
plt.show()

plt.plot(mces.epsilons)
plt.ylabel('Epsilon value')
plt.ylabel('Epsilon value')
print("Epsilones vs episodes")
plt.show()
```

```{python}
mdp = agent1.get_MDP()
# mdp.mdp_graph.view()
mdp
```

```{python}
print("Last episode")
agent1.last_episode
```

```{python}
print("Action-value function")
agent1.action_value
```

```{python}
print("Policy")
agent1.policy
```

```{python}
print("Returns for (State, Action) pairs")
agent1.returns
```

```{python}
# lens = []
# for state, actions in agent1.returns.returns_dict.items():
#     for action, returns in actions.items():
#         lens.append(len(returns))
#         if len(returns) >= 10:
#             print("State:")
#             print(state)
#             print(f"Action: {action}")
#             print(f"Returns: {returns}")
```

```{python}
print("Model")
agent1.model
```

```{python}
# import gym
# import sys
# sys.path.append('/home/jan/WinTacToe/src/extensions/games/tic_tac_toe')
# from tic_tac_toe_logic import *
# players = [Player('A', 0), Player('B', 1)]
# size = 3
# marks_required = 3
# env = gym.make('tic_tac_toe:tictactoe-v0')
# env.initialize(players, size, marks_required)
# s = env.random_initial_state()
# State(s)
```

```{python}
class HumanPlayer():
    def __init__(self, environment, player):
        self.env = environment
        self.player = player
        
    def step(self, state, epsilon=0):
        print("Your opponent move:")
        print(state)
        
        input_string = input("\nType move's coordinates in order y, x (i.e 1,2): ")
        print("\n")
        result = parse("{},{}", input_string)
        y = int(result[0])
        x = int(result[1])
        action = Action([y, x])
        
        raw_state, reward, done, _ = env.step(action.array, self.player) # Take action
        state = State(raw_state)
        print(state)
        
        if reward == 10:
            print("Wygrałeś!!!")
        elif reward == -10:
            print("Przegrałeś!")
        else:
            if done:
                print("Remis!")

        return State(raw_state), done
    
    def reset_episode(self):
        pass
```

```{python}
env = gym.make('tic_tac_toe:tictactoe-v0')
players = [Player('A', 0), Player('B', 1)]
size = 3
marks_required = 3
env.initialize(players, size, marks_required)
# agent1 = Agent(env, players[0])
me = HumanPlayer(env, players[0])
mces = MonteCarloES(agent1, me)
mces.gen_episode(agent1, me, 0)
```

# Wnioski

Nauka agenta przy pomocy podstawowej metody monte carlo gdy drugi gracz ma stałą taktykę (nie uczy się) w kołko i krzyżyk 3x3 wymaga około 2000 epizodów Za stałą taktykę uważamy również taktykę w pełni losową. Gdy drugi agent również się uczy trening wymaga około 150 000 epizodów i skutkuje permanentnym remisem, co oznacza, że gra kółko i krzyżyk 3x3 została "rozwiązana". Warto zwrócić uwagę, że ten agent ani metoda treningu nie są szczególnie wyrafinowane, dlatego można się spodziewać wyższej efektywności po odpowiednich ulepszeniach


# TODO
- [X] Zagrać przeciwko randomowemu graczowi
- [X] Zaimplementować Human Player Agent
- [X] zmienić episode na last_episode w klasie Agent
- [X] Rozegrać gry ze standardowym stanem początkowym
- [X] Dodać MDP
- [X] Stochastic Policy
- [X] Random Policy
- [X] Stochastic Model
- [ ] Dopracować rozgrywkę
- [ ] RL building blocks models
- [ ] Serialization
- [ ] General agent model
- [ ] Project model
- [ ] PyCharm IDE integration
- [ ] Policy merging
- [ ] parallelism support (GPU) -> Numba
- [ ] Better plotting (3D, animations, interactive) -> Blender
- [ ] Distributed computing -> Dask
- [ ] Prometheus integration -> Singularity
- [ ] Lookahead and more advanced RL methods -> Sutton

```{python}

```
